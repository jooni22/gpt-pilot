# Integracja z modelami LLM

## ğŸ§  PrzeglÄ…d systemu LLM

GPT-Pilot obsÅ‚uguje rÃ³Å¼ne modele jÄ™zykowe poprzez ujednolicony interfejs. System jest zaprojektowany tak, aby Å‚atwo dodawaÄ‡ nowe dostawcÃ³w LLM bez wpÅ‚ywu na resztkÄ™ aplikacji.

## ğŸ”Œ ObsÅ‚ugiwane dostawcy

### 1. OpenAI (`openai_client.py`)
**ObsÅ‚ugiwane modele:**
- GPT-4 (zalecany)
- GPT-4 Turbo
- GPT-3.5 Turbo

**Konfiguracja:**
```json
{
  "llm": {
    "provider": "openai",
    "model": "gpt-4",
    "api_key": "your-api-key",
    "temperature": 0.7
  }
}
```

### 2. Anthropic (`anthropic_client.py`)
**ObsÅ‚ugiwane modele:**
- Claude-3 Opus
- Claude-3 Sonnet
- Claude-3 Haiku

**Konfiguracja:**
```json
{
  "llm": {
    "provider": "anthropic",
    "model": "claude-3-sonnet-20240229",
    "api_key": "your-api-key"
  }
}
```

### 3. Groq (`groq_client.py`)
**ObsÅ‚ugiwane modele:**
- Llama 3.1 70B
- Mixtral 8x7B
- Gemma 7B

**Konfiguracja:**
```json
{
  "llm": {
    "provider": "groq",
    "model": "llama-3.1-70b-versatile",
    "api_key": "your-api-key"
  }
}
```### 4. Azure OpenAI (`azure_client.py`)
**ObsÅ‚ugiwane modele:**
- GPT-4 (Azure deployment)
- GPT-3.5 Turbo (Azure deployment)

**Konfiguracja:**
```json
{
  "llm": {
    "provider": "azure",
    "endpoint": "https://your-resource.openai.azure.com/",
    "api_key": "your-api-key",
    "deployment_name": "your-deployment-name"
  }
}
```

## ğŸ—ï¸ Architektura systemu LLM

### Klasa bazowa (`base.py`)
Wszystkie klienty LLM implementujÄ… interfejs `BaseLLMClient`:

```python
class BaseLLMClient:
    def __init__(self, config):
        self.config = config
    
    def send_request(self, messages, **kwargs):
        """WysyÅ‚a Å¼Ä…danie do modelu LLM"""
        raise NotImplementedError
    
    def stream_request(self, messages, **kwargs):
        """WysyÅ‚a Å¼Ä…danie ze streamingiem odpowiedzi"""
        raise NotImplementedError
```

### ZarzÄ…dzanie konwersacjÄ… (`convo.py`)
Klasa `Conversation` zarzÄ…dza historiÄ… rozmowy:
- Przechowuje historiÄ™ wiadomoÅ›ci
- ZarzÄ…dza kontekstem
- Optymalizuje dÅ‚ugoÅ›Ä‡ promptÃ³w

### Parsowanie odpowiedzi (`parser.py`)
System parsowania odpowiedzi od LLM:
- Ekstraktuje kod z odpowiedzi
- Parsuje instrukcje JSON
- Waliduje format odpowiedzi

## ğŸ“ System promptÃ³w

### Struktura promptu (`prompt.py`)
```python
class Prompt:
    def __init__(self, template_path):
        self.template = self.load_template(template_path)
    
    def render(self, **kwargs):
        """Renderuje prompt z danymi"""
        return self.template.format(**kwargs)
```

### Typy promptÃ³w:
- **System prompts** - DefiniujÄ… rolÄ™ agenta
- **Task prompts** - OpisujÄ… konkretne zadanie
- **Context prompts** - DostarczajÄ… kontekst projektu

## ğŸ”§ Konfiguracja

### Plik konfiguracyjny (`config.json`)
```json
{
  "llm": {
    "provider": "openai",
    "model": "gpt-4",
    "api_key": "your-api-key",
    "temperature": 0.7,
    "max_tokens": 4096,
    "timeout": 60,
    "retry_attempts": 3
  }
}
```

### Zmienne Å›rodowiskowe
```bash
export OPENAI_API_KEY="your-openai-key"
export ANTHROPIC_API_KEY="your-anthropic-key"
export GROQ_API_KEY="your-groq-key"
```

## ğŸ“Š Logowanie i monitoring

### Request logging (`request_log.py`)
System loguje wszystkie Å¼Ä…dania do LLM:
- Timestamp Å¼Ä…dania
- UÅ¼yty model
- DÅ‚ugoÅ›Ä‡ promptu
- Czas odpowiedzi
- Koszt Å¼Ä…dania (jeÅ›li dostÄ™pny)

### Metryki wydajnoÅ›ci:
- Åšredni czas odpowiedzi
- Liczba tokenÃ³w uÅ¼ytych
- WspÃ³Å‚czynnik sukcesu Å¼Ä…daÅ„
- Koszty API

## ğŸš€ Optymalizacje

### Cachowanie odpowiedzi
- Cache lokalny dla powtarzajÄ…cych siÄ™ promptÃ³w
- Inteligentne zarzÄ…dzanie pamiÄ™ciÄ… cache

### ZarzÄ…dzanie kontekstem
- Automatyczne skracanie dÅ‚ugich konwersacji
- Priorytetyzacja waÅ¼nych informacji

### Retry logic
- Automatyczne ponowne prÃ³by przy bÅ‚Ä™dach
- Exponential backoff dla rate limiting

## ğŸ”’ BezpieczeÅ„stwo

### ZarzÄ…dzanie kluczami API
- Bezpieczne przechowywanie credentials
- Rotacja kluczy API
- Walidacja uprawnieÅ„

### Filtrowanie treÅ›ci
- Walidacja promptÃ³w przed wysÅ‚aniem
- Filtrowanie potencjalnie szkodliwych treÅ›ci
- Monitoring nietypowych Å¼Ä…daÅ„

## ğŸ› ï¸ RozwiÄ…zywanie problemÃ³w

### Typowe bÅ‚Ä™dy:
1. **Rate limiting** - Zbyt wiele Å¼Ä…daÅ„
2. **Token limit exceeded** - Prompt zbyt dÅ‚ugi
3. **Authentication failed** - NieprawidÅ‚owy klucz API
4. **Model not available** - Model niedostÄ™pny

### Debugowanie:
```python
# WÅ‚Ä…czenie szczegÃ³Å‚owego logowania
logging.getLogger('llm').setLevel(logging.DEBUG)

# Test poÅ‚Ä…czenia z LLM
client = OpenAIClient(config)
response = client.send_request([{"role": "user", "content": "Hello"}])
```